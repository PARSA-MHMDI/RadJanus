import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

def get_user_choice():
    """
    Continuously prompt for Y or N until valid input is received.
    Returns 'Y' or 'N' as a string.
    """
    while True:
        choice = input("Use default settings? Yes [Y] / No [N]: ").strip().upper()
        if choice in ["Y", "N"]:
            return choice
        print("Invalid input. Please type 'Y' or 'N'.")

def get_soup(url, session):
    """
    Return a BeautifulSoup object for the HTML at `url`,
    fetched with the given authenticated session.
    Raises HTTPError if non-200 response.
    """
    resp = session.get(url, timeout=30)
    resp.raise_for_status()
    return BeautifulSoup(resp.text, 'html.parser')

def relative_local_path(full_url):
    """
    Convert a full URL like
      https://physionet.org/files/mimic-cxr-jpg/2.1.0/files/p10/...
    into a relative path like:
      files/p10/...
    so that we preserve the original structure in SAVE_DIR.

    Assumes the portion before 'mimic-cxr-jpg/2.1.0/' is not needed locally.
    """
    split_key = "mimic-cxr-jpg/2.1.0/"
    parts = full_url.split(split_key, 1)
    if len(parts) < 2:
        # If we can't split on that substring, just return the basename
        return os.path.basename(full_url)
    return parts[1].lstrip("/")

def crawl_and_download(session, start_url, save_dir, max_images=1000):
    """
    Recursively crawls subfolders starting from `start_url`.
    Downloads up to `max_images` .jpg files, preserving folder structure
    under `save_dir`.
    """
    images_downloaded = 0
    links_to_crawl = [start_url]
    visited = set()

    while links_to_crawl and images_downloaded < max_images:
        current_url = links_to_crawl.pop()
        if current_url in visited:
            continue
        visited.add(current_url)

        soup = get_soup(current_url, session)

        for link in soup.find_all('a'):
            href = link.get('href')
            if not href or href == '../':
                continue

            full_url = urljoin(current_url, href)

            # If it ends with '/', assume it's a subfolder
            if href.endswith('/'):
                links_to_crawl.append(full_url)
            else:
                # Possibly a file. Download if it ends with .jpg
                if href.lower().endswith('.jpg'):
                    rel_path = relative_local_path(full_url)
                    local_path = os.path.join(save_dir, rel_path)
                    local_dir = os.path.dirname(local_path)
                    os.makedirs(local_dir, exist_ok=True)

                    try:
                        r = session.get(full_url, stream=True)
                        r.raise_for_status()
                        with open(local_path, 'wb') as f:
                            for chunk in r.iter_content(chunk_size=8192):
                                if chunk:
                                    f.write(chunk)
                        images_downloaded += 1
                        print(f"{images_downloaded}: {local_path}")
                        if images_downloaded >= max_images:
                            print("Reached max_images limit.")
                            return
                    except Exception as e:
                        print(f"Failed to download {full_url}: {e}")

def main():
    # Prompt user for default or custom settings
    choice = get_user_choice()

    if choice == "N":
        # Custom input
        sessionid = input("Please enter sessionid: ").strip()
        save_dir = input("Please enter SAVE_DIR (output folder): ").strip()
        try:
            max_images = int(input("Please enter MAX_IMAGES: ").strip())
        except ValueError:
            print("Invalid integer for MAX_IMAGES. Using default=1000.")
            max_images = 10000

        # Hard-code the BASE_URL if you wish, or prompt user as well
        base_url = "https://physionet.org/files/mimic-cxr-jpg/2.1.0/files/"
        pn_cookies = {"sessionid": sessionid}
    else:
        # Default settings
        pn_cookies = {'sessionid': '53moqg0un3pjfkilop0100x2xamn25jr'}  # <--- Example
        base_url = "https://physionet.org/files/mimic-cxr-jpg/2.1.0/files/"
        save_dir = "./Dataset"
        max_images = 1000

    # Create session and add cookies
    session = requests.Session()
    session.cookies.update(pn_cookies)

    # Test the login/permission by requesting the top-level "files" URL
    test_resp = session.get(base_url)
    if test_resp.status_code == 200:
        print("Successful login to PhysioNet. Starting download...")
    else:
        print(f"Login failed with status code: {test_resp.status_code}")
        print("Please confirm you have a valid 'sessionid' and have satisfied all PhysioNet credential requirements.")
        return

    # Start the crawling/downloading
    crawl_and_download(session, base_url, save_dir, max_images)
    print("Done.")

# If you'd like to run automatically in a script, uncomment below:
if __name__ == "__main__":
    main()

